## General Purpose of this repository
This project contains all script to prepare the data and train on this, including the model and training script to predict the Open reading frame in ancient DNA samples from Exons and without additional damage.

### Data Preparation
The data for the training of the models is simulated using tools such as Gargammel, seqkit, leeHom and art_illumina to create reads from next generation sequencing. The script generate_reads.sh in data_preparation can be used to generate these where the read length and the number of fragments can be defined. Further the directory containing the genomes of the organisms (from ncbi) should be given. Additionally, the number of organisms from which the fragments should be generated can be given by defining max_dirs.
The data should be zipped after its generation and then encode_numbers.py can be used to one hot encode and prepare the data suitable for the neural network. After that the files should be zipped and chuncks should be created to avoid an exploding RAM during the training. Therefore chunck_script should be used for the training and validation data to split these in a given amount of subsets. Chunck script creates the chuncks in the same directory like the original datasets. So only the path to this directory is needed to indicate the dataset on which the model should be trained on.

### Training
The model is fairly big and if it is trained on more than 1000 organisms with more than 1000 fragments per organism it should be trained on high performance graphic nodes.  Tesla A100 PCIE 80 GB on an LSF cluster was used to train the model and for that the modules are defined in requirements.txt. Probably, these will differ for different architectures so they should be adjusted if other graphic cards are used. jobscript_big.sh was used to submit the jobs which is based on the script training.py. There one can define all relevant parameters for the training such as optimizer, batch size, data directory etc. 